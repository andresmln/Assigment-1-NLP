{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1ff66b-76d5-4d66-988e-31213289f62e",
   "metadata": {},
   "source": [
    "**First we load and merge train, val and test datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95206848-e7d9-4cd7-a856-0817772dbfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Examples: 8865\n",
      "  Argument ID                                   Conclusion       Stance  \\\n",
      "0      A01002                  We should ban human cloning  in favor of   \n",
      "1      A01005                      We should ban fast food  in favor of   \n",
      "2      A01006  We should end the use of economic sanctions      against   \n",
      "\n",
      "                                             Premise  Self-direction: thought  \\\n",
      "0  we should ban human cloning as it will only ca...                        0   \n",
      "1  fast food should be banned because it is reall...                        0   \n",
      "2  sometimes economic sanctions are the only thin...                        0   \n",
      "\n",
      "   Self-direction: action  Stimulation  Hedonism  Achievement  \\\n",
      "0                       0            0         0            0   \n",
      "1                       0            0         0            0   \n",
      "2                       0            0         0            0   \n",
      "\n",
      "   Power: dominance  ...  Tradition  Conformity: rules  \\\n",
      "0                 0  ...          0                  0   \n",
      "1                 0  ...          0                  0   \n",
      "2                 1  ...          0                  0   \n",
      "\n",
      "   Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
      "0                          0         0                    0   \n",
      "1                          0         0                    0   \n",
      "2                          0         0                    0   \n",
      "\n",
      "   Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
      "0                           0                      0                     0   \n",
      "1                           0                      0                     0   \n",
      "2                           0                      0                     0   \n",
      "\n",
      "   Universalism: tolerance  Universalism: objectivity  \n",
      "0                        0                          0  \n",
      "1                        0                          0  \n",
      "2                        0                          0  \n",
      "\n",
      "[3 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load Training Data\n",
    "train_args = pd.read_csv(\"data/arguments-training.tsv\", sep='\\t')\n",
    "train_labels = pd.read_csv(\"data/labels-training.tsv\", sep='\\t')\n",
    "df_train = pd.merge(train_args, train_labels, on=\"Argument ID\")\n",
    "\n",
    "# 2. Load Validation Data\n",
    "val_args = pd.read_csv(\"data/arguments-validation.tsv\", sep='\\t')\n",
    "val_labels = pd.read_csv(\"data/labels-validation.tsv\", sep='\\t')\n",
    "df_val = pd.merge(val_args, val_labels, on=\"Argument ID\")\n",
    "\n",
    "# 3. Load Test Data (Crucial for volume!)\n",
    "test_args = pd.read_csv(\"data/arguments-test.tsv\", sep='\\t')\n",
    "test_labels = pd.read_csv(\"data/labels-test.tsv\", sep='\\t')\n",
    "df_test = pd.merge(test_args, test_labels, on=\"Argument ID\")\n",
    "\n",
    "# 4. Concatenate EVERYTHING into one giant dataset\n",
    "trainval_df = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "\n",
    "# 5. Verify the size (Should be > 8,500)\n",
    "print(f\"Total Examples: {len(trainval_df)}\")\n",
    "print(trainval_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4d33d-5d37-4e00-b32a-d832bcd4385b",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8bee10-1cf5-47e7-a19c-2cd51ebbacba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜî ID: A28426\n",
      "üì¢ CONCLUSION: Payday loans should be banned\n",
      "‚öñÔ∏è STANCE: in favor of\n",
      "üìù PREMISE: payday loans should be banned because it causes people to go into debt\n",
      "------------------------------\n",
      "üß† ACTUAL HUMAN VALUES (Ground Truth):\n",
      "   ‚úÖ Power: resources\n",
      "   ‚úÖ Security: personal\n",
      "================================================================================\n",
      "\n",
      "üÜî ID: A21315\n",
      "üì¢ CONCLUSION: Homeopathy brings more harm than good\n",
      "‚öñÔ∏è STANCE: in favor of\n",
      "üìù PREMISE: introducing items that normally produce symptoms of a disease is something that really could do more harm than good in the long run.\n",
      "------------------------------\n",
      "üß† ACTUAL HUMAN VALUES (Ground Truth):\n",
      "   ‚úÖ Security: personal\n",
      "   ‚úÖ Universalism: objectivity\n",
      "================================================================================\n",
      "\n",
      "üÜî ID: A25015\n",
      "üì¢ CONCLUSION: Payday loans should be banned\n",
      "‚öñÔ∏è STANCE: in favor of\n",
      "üìù PREMISE: payday loans allow people to spend money they do not have yet and then they have to pay interest on the loan.  this could cause them to need another loan to get through the next pay period.\n",
      "------------------------------\n",
      "üß† ACTUAL HUMAN VALUES (Ground Truth):\n",
      "   ‚úÖ Power: resources\n",
      "   ‚úÖ Security: personal\n",
      "   ‚úÖ Universalism: concern\n",
      "================================================================================\n",
      "\n",
      "üÜî ID: A28416\n",
      "üì¢ CONCLUSION: Intelligence tests bring more harm than good\n",
      "‚öñÔ∏è STANCE: in favor of\n",
      "üìù PREMISE: intelligence tests could label a child as unintelligent when they are not fully developed and so stunt their intellectual growth by placing them in lower levels\n",
      "------------------------------\n",
      "üß† ACTUAL HUMAN VALUES (Ground Truth):\n",
      "   ‚úÖ Achievement\n",
      "   ‚úÖ Face\n",
      "   ‚úÖ Universalism: concern\n",
      "================================================================================\n",
      "\n",
      "üÜî ID: A21211\n",
      "üì¢ CONCLUSION: Entrapment should be legalized\n",
      "‚öñÔ∏è STANCE: in favor of\n",
      "üìù PREMISE: entrapment should be legalized because it gets a lot of the bad guys off the streets\n",
      "------------------------------\n",
      "üß† ACTUAL HUMAN VALUES (Ground Truth):\n",
      "   ‚úÖ Security: societal\n",
      "   ‚úÖ Conformity: rules\n",
      "   ‚úÖ Universalism: concern\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the Data\n",
    "# Using validation set because it's cleaner for inspection\n",
    "df_args = pd.read_csv(\"data/arguments-validation.tsv\", sep='\\t')\n",
    "df_labels = pd.read_csv(\"data/labels-validation.tsv\", sep='\\t')\n",
    "\n",
    "# 2. Merge them\n",
    "val_df = pd.merge(df_args, df_labels, on=\"Argument ID\")\n",
    "\n",
    "# 3. Identify the Value Columns (The 19 or 20 labels)\n",
    "# We exclude the text columns to find just the label columns\n",
    "metadata_cols = ['Argument ID', 'Conclusion', 'Stance', 'Premise', 'Language']\n",
    "value_cols = [col for col in val_df.columns if col not in metadata_cols]\n",
    "\n",
    "# 4. Display 5 Random Examples\n",
    "# Change random_state to see different examples\n",
    "samples = val_df.sample(5, random_state=42) \n",
    "\n",
    "for idx, row in samples.iterrows():\n",
    "    print(f\"üÜî ID: {row['Argument ID']}\")\n",
    "    print(f\"üì¢ CONCLUSION: {row['Conclusion']}\")\n",
    "    print(f\"‚öñÔ∏è STANCE: {row['Stance']}\")\n",
    "    print(f\"üìù PREMISE: {row['Premise']}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"üß† ACTUAL HUMAN VALUES (Ground Truth):\")\n",
    "    \n",
    "    # Iterate through the columns and print only the ones marked as '1'\n",
    "    has_values = False\n",
    "    for val in value_cols:\n",
    "        if row[val] == 1:\n",
    "            print(f\"   ‚úÖ {val}\")\n",
    "            has_values = True\n",
    "            \n",
    "    if not has_values:\n",
    "        print(\"   (No values annotated)\")\n",
    "        \n",
    "    print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455bbe9-a4a9-4d8b-a653-a7f33e4c4cfd",
   "metadata": {},
   "source": [
    "**Use iterative-stratification library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f02b65-07d9-46a5-83e2-8fc5587882e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00857a72-d4f0-4e8b-84d5-d8d64b2f7438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (8865,)\n",
      "Labels shape:   (8865, 20)\n",
      "------------------------------\n",
      "Final Training Set: 7112 examples (Use for Cross-Validation)\n",
      "Final Test Set:     1753 examples (Use for Report)\n",
      "\n",
      "Label Distribution Check (First 3 labels):\n",
      "Train: [0.15551181 0.25674916 0.05202475]\n",
      "Test:  [0.15744438 0.2601255  0.05248146]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "# 1. Create the Input Feature (X) and Targets (y) from your NEW trainval_df\n",
    "# Concatenate Conclusion + Stance + Premise\n",
    "trainval_df['text'] = trainval_df['Conclusion'] + \" \" + trainval_df['Stance'] + \" \" + trainval_df['Premise']\n",
    "\n",
    "label_cols = [col for col in train_labels.columns if col != 'Argument ID']\n",
    "\n",
    "# Create the arrays for splitting\n",
    "X_all = trainval_df['text'].values\n",
    "y_all = trainval_df[label_cols].values\n",
    "\n",
    "print(f\"Features shape: {X_all.shape}\")\n",
    "print(f\"Labels shape:   {y_all.shape}\")\n",
    "\n",
    "# 2. Iterative Stratified Split (Train vs Test)\n",
    "# We use X_all and y_all here\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# FIX: Use 'X_all' and 'y_all' inside the loop\n",
    "for train_index, test_index in msss.split(X_all, y_all):\n",
    "    X_train, X_test = X_all[train_index], X_all[test_index]\n",
    "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Final Training Set: {X_train.shape[0]} examples (Use for Cross-Validation)\")\n",
    "print(f\"Final Test Set:     {X_test.shape[0]} examples (Use for Report)\")\n",
    "\n",
    "# OPTIONAL: Sanity Check\n",
    "print(\"\\nLabel Distribution Check (First 3 labels):\")\n",
    "print(f\"Train: {np.mean(y_train, axis=0)[:3]}\")\n",
    "print(f\"Test:  {np.mean(y_test, axis=0)[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab54c150-3abe-4b02-b799-51af266d77ff",
   "metadata": {},
   "source": [
    "**SPARSE REPRESENTATION**\n",
    "\n",
    "We compare TF-IDF approach with CountVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dba088e-27ce-4aa0-a759-f1da610fd378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Type | N-Grams    | Mean F1-Macro   | Std Dev   \n",
      "------------------------------------------------------------\n",
      "TF-IDF       | (1, 1)     | 0.2617          | 0.0039\n",
      "TF-IDF       | (1, 2)     | 0.2693          | 0.0057\n",
      "TF-IDF       | (1, 3)     | 0.2823          | 0.0076\n",
      "CountVec     | (1, 1)     | 0.3962          | 0.0103\n",
      "CountVec     | (1, 2)     | 0.4215          | 0.0086\n",
      "CountVec     | (1, 3)     | 0.4250          | 0.0080\n",
      "------------------------------------------------------------\n",
      "üèÜ WINNER: CountVec (1, 3) with F1-Macro: 0.4250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# 1. Setup the Cross-Validation Strategy (Mandatory)\n",
    "# Matches your data splitting logic (Stratified Multi-label)\n",
    "stratified_cv = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 2. Define Experiments\n",
    "# We test different Feature Types AND N-gram ranges\n",
    "experiments = [\n",
    "    (\"TF-IDF\", (1, 1)),      # Standard baseline\n",
    "    (\"TF-IDF\", (1, 2)),      # Captures phrases (\"climate change\")\n",
    "    (\"TF-IDF\", (1, 3)),      # Captures longer context\n",
    "    (\"CountVec\", (1, 1)),    # Raw frequency (Bag of Words)\n",
    "    (\"CountVec\", (1, 2)),    # Raw frequency + Phrases\n",
    "    (\"CountVec\", (1, 3)),    # Trigrams\n",
    "]\n",
    "\n",
    "print(f\"{'Feature Type':<12} | {'N-Grams':<10} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_score = 0\n",
    "best_config = \"\"\n",
    "\n",
    "for vec_type, ngram in experiments:\n",
    "    # 3. Select Vectorizer\n",
    "    if vec_type == \"TF-IDF\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=ngram, min_df=3, max_features=20000)\n",
    "        # vectorizer = TfidfVectorizer(ngram_range=ngram, analyzer = 'char', min_df=3, max_features=20000) # If we want to analyze tokens = char\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram, min_df=3, max_features=20000)\n",
    "        # vectorizer = CountVectorizer(ngram_range=ngram, analyzer = 'char', min_df=3, max_features=20000) # If we want to analyze tokens = char\n",
    "        \n",
    "    # 4. Build Pipeline\n",
    "    # Using Logistic Regression (OneVsRest) as the standard baseline classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('vec', vectorizer),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42)))\n",
    "    ])\n",
    "    \n",
    "    # 5. Run Cross-Validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=-1)\n",
    "    \n",
    "    # 6. Store Results\n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    \n",
    "    print(f\"{vec_type:<12} | {str(ngram):<10} | {mean_score:.4f}          | {std_score:.4f}\")\n",
    "    \n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_config = f\"{vec_type} {ngram}\"\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"üèÜ WINNER: {best_config} with F1-Macro: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca8c7e-ffab-4b92-aa3d-744691ea6fcb",
   "metadata": {},
   "source": [
    "*Feature representation analysis:*\n",
    "\n",
    "- Our experiments revealed that CountVectors (Raw Frequency) significantly outperformed TF-IDF. This suggests that for short argumentation texts, the raw presence of specific value-laden keywords (e.g., 'freedom', 'security') is the most predictive feature.\n",
    "\n",
    "- TF-IDF attempts to down-weight common terms, but in this domain, high-frequency terms are often the exact class identifiers we need. Since BM25 is mathematically an extension of TF-IDF (designed to further penalize term saturation and normalize length), it inherits the same 'flaw' for this specific dataset.\n",
    "\n",
    "- Consequently, because the simpler CountVectors model already outperforms the weighted TF-IDF model by a large margin, we conclude that complex frequency dampening (like that in BM25) is unnecessary and detrimental for this specific task. We therefore selected CountVectors (N-gram 1,2) as our optimal Sparse baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b70dc9-e697-494f-8569-a9054cc64d14",
   "metadata": {},
   "source": [
    "**Dense Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8154dc2-a4a8-4879-81dc-ca72e0645573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import gensim.downloader as api\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # 1. Define the Vectorizer (Averaging Logic)\n",
    "# class MeanEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, model_name):\n",
    "#         self.model_name = model_name\n",
    "#         self.word2vec = None\n",
    "#         self.dim = None\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         # Load the model only when fitting to save memory/time if not used\n",
    "#         print(f\"Loading {self.model_name}...\")\n",
    "#         self.word2vec = api.load(self.model_name)\n",
    "#         self.dim = self.word2vec.vector_size\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         # Check if model is loaded\n",
    "#         if self.word2vec is None:\n",
    "#              self.word2vec = api.load(self.model_name)\n",
    "#              self.dim = self.word2vec.vector_size\n",
    "             \n",
    "#         return np.array([\n",
    "#             np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "#                     or [np.zeros(self.dim)], axis=0)\n",
    "#             for words in [s.lower().split() for s in X]\n",
    "#         ])\n",
    "\n",
    "# # 2. Define the Models to Compare\n",
    "# # Format: (Display Name, Gensim API Name)\n",
    "# dense_models = [\n",
    "#     (\"GloVe (100d)\", \"glove-wiki-gigaword-100\"),\n",
    "#     (\"Word2Vec (300d)\", \"word2vec-google-news-300\") \n",
    "# ]\n",
    "\n",
    "# print(f\"{'Model Name':<20} | {'Dimensions':<10} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "# print(\"-\" * 65)\n",
    "\n",
    "# results_dense = {}\n",
    "\n",
    "# for display_name, api_name in dense_models:\n",
    "#     # 3. Build Pipeline\n",
    "#     # We initialize the vectorizer with the model name, it loads during fit()\n",
    "#     pipeline = Pipeline([\n",
    "#         ('vec', MeanEmbeddingVectorizer(api_name)),\n",
    "#         ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42)))\n",
    "#     ])\n",
    "    \n",
    "#     # 4. Run Cross-Validation\n",
    "#     # Note: This might be slower due to the large matrix operations in 300d\n",
    "#     scores = cross_val_score(pipeline, X_train, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=-1)\n",
    "    \n",
    "#     # 5. Store and Print\n",
    "#     mean_score = scores.mean()\n",
    "#     results_dense[display_name] = mean_score\n",
    "#     print(f\"{display_name:<20} | {str(300 if '300' in api_name else 100):<10} | {mean_score:.4f}          | {scores.std():.4f}\")\n",
    "\n",
    "# # 6. Final Comparison\n",
    "# best_dense = max(results_dense, key=results_dense.get)\n",
    "# print(\"-\" * 65)\n",
    "# print(f\"üèÜ Best Dense Model: {best_dense} with F1: {results_dense[best_dense]:.4f}\")\n",
    "\n",
    "# # Optional: Compare against your Sparse Baseline (assuming 'best_score' exists)\n",
    "# try:\n",
    "#     print(f\"Sparse Baseline:     {best_score:.4f}\")\n",
    "#     if results_dense[best_dense] > best_score:\n",
    "#         print(\"üöÄ Result: Dense Embeddings BEAT Sparse Features!\")\n",
    "#     else:\n",
    "#         print(\"üìâ Result: Sparse Features (CountVec/TF-IDF) are SUPERIOR.\")\n",
    "# except NameError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34a586-f368-4f1a-87dc-a244cb2e240c",
   "metadata": {},
   "source": [
    "# 3. Required Experiments & Ablation Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4f0b8-c252-488e-a9ae-87d9e6357d5c",
   "metadata": {},
   "source": [
    "# N-Gramas solo tiene sentido con Sparse "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f057a-61f7-41fd-b4d2-a7ba23ebc94e",
   "metadata": {},
   "source": [
    "No se aplica N-gramas para dense ya que glove tiene embeddings de 1 palabra y nunca van a ser de 2 palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e3bbed-29fa-4186-844e-acfedf6a6019",
   "metadata": {},
   "source": [
    "## Preprocessing Ablation: Test the impact of cleaning strategies (Raw vs. Lowercase, Stopword removal, Lemmatization, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df385861-76e5-44e2-9ce8-be70e574bb71",
   "metadata": {},
   "source": [
    "### Raw vs Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a237f2-b3f2-42b9-9bef-45862abf2896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVec (1, 3)\n",
      "Configuraci√≥n aplicada: (1, 3)\n",
      "Estrategia                               | Mean F1-Macro   | Std Dev   \n",
      "----------------------------------------------------------------------\n",
      "Raw (Mantiene may√∫sculas y min√∫sculas)   | 0.4268          | 0.0046\n",
      "Lowercase (Todo a min√∫sculas)            | 0.4250          | 0.0080\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "print(best_config)\n",
    "numeros = re.findall(r'\\d+', best_config)\n",
    "\n",
    "best_ngram_params = tuple(map(int, numeros))\n",
    "\n",
    "print(f\"Configuraci√≥n aplicada: {best_ngram_params}\")\n",
    "# 1. Definimos los dos experimentos: Raw vs Lowercase\n",
    "# Usamos CountVectorizer con best_ngram_params =(1, 3) que fue la mejor representaci√≥n\n",
    "experimentos_casing = [\n",
    "    (\n",
    "        \"Raw (Mantiene may√∫sculas y min√∫sculas)\", \n",
    "        CountVectorizer(ngram_range= best_ngram_params, lowercase=False, min_df=3, max_features=20000) #Camb\n",
    "    ),\n",
    "    (\n",
    "        \"Lowercase (Todo a min√∫sculas)\", \n",
    "        CountVectorizer(ngram_range = best_ngram_params, lowercase=True, min_df=3, max_features=20000) # Camb\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"{'Estrategia':<40} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# 2. Ejecutamos la comparaci√≥n\n",
    "for nombre, vectorizador in experimentos_casing:\n",
    "    # Construimos el pipeline igual que en tus experimentos anteriores\n",
    "    pipeline = Pipeline([\n",
    "        ('vec', vectorizador),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42)))\n",
    "    ])\n",
    "    \n",
    "    # Ejecutamos cross-validation (aseg√∫rate de que X_train, y_train y stratified_cv est√°n definidos)\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=-1)\n",
    "    \n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    \n",
    "    print(f\"{nombre:<40} | {mean_score:.4f}          | {std_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40719cf6-f07f-4214-9a79-dfcc2c0d0fef",
   "metadata": {},
   "source": [
    "## Lemmatization + Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77efc24-8667-4879-910c-338bea994b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Lematizando las palabras del conjunto de entrenamiento...\n",
      "‚úÖ ¬°Lematizaci√≥n completada!\n",
      "\n",
      "Estrategia (Tokens = Palabras)           | Mean F1-Macro   | Std Dev   \n",
      "----------------------------------------------------------------------\n",
      "Word: Lowercase (Baseline sin lematizar) | 0.4250          | 0.0080\n"
     ]
    }
   ],
   "source": [
    "#Func Cambiado\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Preparar las herramientas de NLTK\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Funci√≥n que toma un texto en minusculas, lo divide en tokens (palabras), lematiza cada palabra y las vuelve a unir\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in str(text).lower().split()])\n",
    "\n",
    "# 2. Pre-computar la lematizaci√≥n sobre todo el dataset\n",
    "print(\"‚è≥ Lematizando las palabras del conjunto de entrenamiento...\")\n",
    "X_train_lemmatized = np.array([lemmatize_text(text) for text in X_train])\n",
    "print(\"‚úÖ ¬°Lematizaci√≥n completada!\\n\")\n",
    "\n",
    "# 3. Definir los experimentos expl√≠citamente a nivel de PALABRA\n",
    "# Utilizamos analyzer='word' (aunque es el valor por defecto, es buena pr√°ctica ponerlo tras haber usado caracteres)\n",
    "experimentos_palabras = [\n",
    "    (\n",
    "        \"Word: Lowercase (Baseline sin lematizar)\", \n",
    "        CountVectorizer(analyzer='word', ngram_range=(1, 3), lowercase=True, min_df=3, max_features=20000),\n",
    "        X_train  # Pasamos el texto original\n",
    "    ),\n",
    "    (\n",
    "        \"Word: Lowercase + Lematizaci√≥n\", \n",
    "        CountVectorizer(analyzer='word', ngram_range=(1, 3), lowercase=True, min_df=3, max_features=20000),\n",
    "        X_train_lemmatized  # Pasamos el texto con las palabras lematizadas\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"{'Estrategia (Tokens = Palabras)':<40} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# 4. Ejecutar la validaci√≥n cruzada\n",
    "for nombre, vectorizador, datos_x in experimentos_palabras:\n",
    "    pipeline = Pipeline([\n",
    "        ('vec', vectorizador),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42)))\n",
    "    ])\n",
    "    \n",
    "    # Usamos stratified_cv y y_train (asumiendo que los mantienes definidos de antes)\n",
    "    scores = cross_val_score(pipeline, datos_x, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=-1)\n",
    "    \n",
    "    mean_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "    \n",
    "    print(f\"{nombre:<40} | {mean_score:.4f}          | {std_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd25e6-6595-4c1a-8409-76b8945d95fc",
   "metadata": {},
   "source": [
    "### Stopword removal + Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c882f74-f316-49c3-bb17-03766c817662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 1. Preparar las stopwords de NLTK\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Cargamos las stopwords y las convertimos a lista (CountVectorizer prefiere listas)\n",
    "stop_words_en = list(set(stopwords.words('english')))\n",
    "\n",
    "# 2. Definir los experimentos\n",
    "# Nota: Asumimos que X_train, y_train y stratified_cv ya est√°n definidos en tu notebook\n",
    "experimentos_stopwords = [\n",
    "    (\n",
    "        \"Complete text (With Stopwords)\", \n",
    "        CountVectorizer(ngram_range = best_ngram_params, lowercase = True, min_df = 3, max_features = 20000),\n",
    "        X_train  # Pasamos el texto original\n",
    "    ),\n",
    "    (\n",
    "        \"Complete text (Without Stopwords)\", \n",
    "        # A√±adimos stop_words=stop_words_en directamente al vectorizador\n",
    "        CountVectorizer(ngram_range = best_ngram_params, lowercase = True, stop_words = stop_words_en, min_df = 3, max_features = 20000),\n",
    "        X_train # Pasamos texto original\n",
    "    )\n",
    "]\n",
    "\n",
    "# 3. Imprimir cabecera de resultados\n",
    "print(f\"{'Estrategia':<40} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# 4. Ejecutar la validaci√≥n cruzada para cada experimento\n",
    "for nombre, vectorizador, datos_x in experimentos_stopwords:\n",
    "    \n",
    "    # Creamos el pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vec', vectorizador),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42)))\n",
    "    ])\n",
    "    \n",
    "    # Ejecutamos cross-validation\n",
    "    scores = cross_val_score(pipeline, datos_x, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=-1)\n",
    "    \n",
    "    # Imprimimos los resultados formateados\n",
    "    print(f\"{nombre:<40} | {scores.mean():.4f}          | {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a605ad-7cd1-4e45-81a5-92db445ba9c7",
   "metadata": {},
   "source": [
    "### Experimento Lowercase + Lemma With Stopwords vs Without Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a0acb-5905-4b15-b456-352420d09b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentos_combo = [\n",
    "    (\n",
    "        \"Lowercase + Lemma (CON Stopwords)\", \n",
    "        # Al no poner el par√°metro stop_words, las stopwords se mantienen\n",
    "        CountVectorizer(ngram_range=best_ngram_params, lowercase=True, min_df=3, max_features=20000),\n",
    "        X_train_lemmatized  \n",
    "    ),\n",
    "    (\n",
    "        \"Lowercase + Lemma (SIN Stopwords)\", \n",
    "        # Aqu√≠ le pasamos la lista para que elimine las stopwords\n",
    "        CountVectorizer(ngram_range=best_ngram_params, lowercase=True, stop_words=stop_words_en, min_df=3, max_features=20000),\n",
    "        X_train_lemmatized  \n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"{'Estrategia':<40} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for nombre, vectorizador, datos_x in experimentos_combo:\n",
    "    pipeline = Pipeline([\n",
    "        ('vec', vectorizador),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42)))\n",
    "    ])\n",
    "    \n",
    "    # Ejecutamos cross-validation\n",
    "    scores = cross_val_score(pipeline, datos_x, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=-1)\n",
    "    \n",
    "    print(f\"{nombre:<40} | {scores.mean():.4f}          | {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5933a-b0f0-48d3-995c-8a54fbc65eef",
   "metadata": {},
   "source": [
    "We obtain the best results with the combination of **Lowercase + Lemma + removing Stopwords**. We obtain a Mean F1-Macro $= 0.4369 $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40c58d-c8d2-47c4-990e-9ed1fddec90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. Definir el pipeline \"ganador\"\n",
    "# Utiliza aqu√≠ la configuraci√≥n (con o sin stopwords) que mejor resultado te dio en el paso anterior.\n",
    "pipeline_optimizar = Pipeline([\n",
    "    ('vec', CountVectorizer(lowercase=True, stop_words=stop_words_en, max_features=20000)),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42), max_iter=2000))\n",
    "])\n",
    "\n",
    "# 2. Definir la cuadr√≠cula (Grid) de hiperpar√°metros a probar\n",
    "param_grid = {\n",
    "    # Regularizaci√≥n de la regresi√≥n log√≠stica\n",
    "    'clf__estimator__C': [0.1, 1.0, 5.0, 10.0],\n",
    "    \n",
    "    # N-gramas: probar si el contexto de 2 o 3 palabras ayuda\n",
    "    'vec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    \n",
    "    # min_df: 1 (sin filtro), 3 y 5 (filtro suave absoluto), 0.01 (filtro proporcional del 1%)\n",
    "    'vec__min_df': [1, 3, 5, 0.01],\n",
    "    \n",
    "    # max_df: 1.0 (sin filtro), 0.90 (quita palabras en el 90% de los textos), 0.75 (m√°s agresivo)\n",
    "    'vec__max_df': [0.75, 0.90, 1.0]\n",
    "}\n",
    "\n",
    "# 3. Configurar el GridSearchCV\n",
    "print(\"‚è≥ Iniciando GridSearchCV (esto entrenar√° m√∫ltiples modelos y tomar√° tiempo)...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_optimizar,\n",
    "    param_grid=param_grid,\n",
    "    cv=stratified_cv,        # Usamos tu misma validaci√≥n cruzada\n",
    "    scoring='f1_macro',      # Optimizamos para F1-Macro\n",
    "    n_jobs=-1,               # Usamos todos los n√∫cleos de tu CPU\n",
    "    verbose=1                # Muestra por pantalla que hiperparametros esta probando en este momento\n",
    ")\n",
    "\n",
    "# 4. Entrenar y buscar (asumiendo que X_train_lemmatized fue tu mejor texto)\n",
    "grid_search.fit(X_train_lemmatized, y_train)\n",
    "\n",
    "# 5. Imprimir los resultados triunfales\n",
    "print(\"\\n‚úÖ ¬°Optimizaci√≥n completada!\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"üèÜ Mejor F1-Macro obtenido: {grid_search.best_score_:.4f}\")\n",
    "print(\"üîß Mejores Hiperpar√°metros encontrados:\")\n",
    "\n",
    "# Imprimir los par√°metros ordenados de forma bonita\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(f\"  * {param_name}: {grid_search.best_params_[param_name]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200f86c6-2a01-445f-b4ef-76a0e6b0c292",
   "metadata": {},
   "source": [
    "üèÜ Mejor F1-Macro obtenido: 0.4418\n",
    "üîß Mejores Hiperpar√°metros encontrados:\n",
    "  * clf__estimator__C: 5.0\n",
    "  * vec__max_df: 0.75\n",
    "  * vec__min_df: 3\n",
    "  * vec__ngram_range: (1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ea4abd-fa6d-44da-a377-5cdf9c974f82",
   "metadata": {},
   "source": [
    "# 4. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f4499-3306-4560-afde-af8aec94546d",
   "metadata": {},
   "source": [
    "Se ha aplicado el mismo preprocesamiento que en los datos de train (lowercase + Lemmatization y quitar stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7cf13-e554-4f74-87dc-f906988ad9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 1. La misma funci√≥n exacta que usamos para el entrenamiento\n",
    "def lemmatize_text(text):\n",
    "    # Pasa a min√∫sculas (.lower()), divide y lematiza\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in str(text).lower().split()])\n",
    "\n",
    "# 2. Aplicar la transformaci√≥n a tu X_test original\n",
    "print(\"‚è≥ Aplicando lowercase y lematizaci√≥n al conjunto de test...\")\n",
    "X_test_lemmatized = np.array([lemmatize_text(text) for text in X_test])\n",
    "print(\"‚úÖ ¬°Test preprocesado con √©xito y listo para predecir!\")\n",
    "\n",
    "# Comprobaci√≥n r√°pida para que veas c√≥mo qued√≥:\n",
    "print(\"\\nüîç Ejemplo del antes y despu√©s:\")\n",
    "print(f\"Original:   {X_test[0] if len(X_test) > 0 else 'N/A'}\")\n",
    "print(f\"Procesado:  {X_test_lemmatized[0] if len(X_test_lemmatized) > 0 else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40168ec-1b31-4c8a-acc7-b9a5571be52b",
   "metadata": {},
   "source": [
    "## 4.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60b5de-860d-4972-9830-33605ab86a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# 1. Generate predictions on the test set\n",
    "print(\"Generating predictions on the test set...\")\n",
    "y_pred = grid_search.predict(X_test_lemmatized)\n",
    "\n",
    "# 2. Calculate the multi-label confusion matrix (20 matrices of 2x2)\n",
    "mcm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 3. EXTRAER LOS NOMBRES DE LAS VARIABLES (Ignorando 'Argument ID')\n",
    "# Usamos las columnas de train_labels pero filtramos el ID\n",
    "class_names = [col for col in train_labels.columns if col != 'Argument ID']\n",
    "\n",
    "# 4. Configure the plot layout\n",
    "num_classes = len(class_names)\n",
    "cols = 4  # Ponemos 4 columnas para que quepan mejor las 20 clases (5 filas x 4 columnas)\n",
    "rows = math.ceil(num_classes / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n",
    "axes = axes.flatten() \n",
    "\n",
    "# 5. Plot each 2x2 matrix\n",
    "for i, (matrix, name) in enumerate(zip(mcm, class_names)):\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'],\n",
    "                cbar=False)\n",
    "    \n",
    "    # T√≠tulo con el nombre real del valor (ej. \"Hedonism\")\n",
    "    axes[i].set_title(f'{name}', fontweight='bold', fontsize=12)\n",
    "    axes[i].set_ylabel('True Label')\n",
    "    axes[i].set_xlabel('Predicted Label')\n",
    "\n",
    "# Ocultar los gr√°ficos vac√≠os si sobran cuadr√≠culas\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f682e9-c100-4d88-a5c3-a8a80586b6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06db940-708d-450d-b794-70a846192a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a546e35-13d2-4124-b0c4-d975e7402422",
   "metadata": {},
   "source": [
    "## 4.2 Discriminative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec302ce-9618-4c4f-84d9-acf1d7d17a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Extraer los componentes del pipeline\n",
    "mejor_pipeline = grid_search.best_estimator_\n",
    "vectorizador = mejor_pipeline.named_steps['vec']\n",
    "modelo_ovr = mejor_pipeline.named_steps['clf']\n",
    "\n",
    "# 2. Obtener el vocabulario y los modelos\n",
    "nombres_features = vectorizador.get_feature_names_out()\n",
    "clasificadores = modelo_ovr.estimators_\n",
    "\n",
    "# 3. Nombres de tus clases\n",
    "class_names = [col for col in train_labels.columns if col != 'Argument ID']\n",
    "\n",
    "# --- NUEVO: DIAGN√ìSTICO DE TAMA√ëOS ---\n",
    "print(\"‚öôÔ∏è DIAGN√ìSTICO DEL MODELO:\")\n",
    "print(f\" - N√∫mero de nombres de variables: {len(class_names)}\")\n",
    "print(f\" - N√∫mero de modelos entrenados: {len(clasificadores)}\")\n",
    "print(\"=\" * 70)\n",
    "# ------------------------------------\n",
    "\n",
    "print(\"\\nüîç EXTRACCI√ìN DE DISCRIMINATIVE FEATURES (TOP 10 N-GRAMAS)\")\n",
    "\n",
    "# 4. Bucle seguro (iteramos solo hasta el n√∫mero real de clasificadores que existen)\n",
    "for i in range(len(clasificadores)):\n",
    "    \n",
    "    # Asignamos el nombre de forma segura\n",
    "    if i < len(class_names):\n",
    "        nombre_clase = class_names[i]\n",
    "    else:\n",
    "        # Por si hay m√°s clasificadores que nombres (muy raro)\n",
    "        nombre_clase = f\"Clase_Desconocida_{i}\"\n",
    "        \n",
    "    # Extraer los coeficientes matem√°ticos de la clase actual\n",
    "    coeficientes = clasificadores[i].coef_[0]\n",
    "    \n",
    "    # Obtener los √≠ndices de las palabras ordenados por su peso\n",
    "    indices_ordenados = np.argsort(coeficientes)\n",
    "    \n",
    "    # Extraer los top n-gramas usando los √≠ndices\n",
    "    top_negativos = [nombres_features[idx] for idx in indices_ordenados[:10]]\n",
    "    top_positivos = [nombres_features[idx] for idx in indices_ordenados[-10:]][::-1]\n",
    "    \n",
    "    print(f\"\\nüèÜ CLASE: {nombre_clase}\")\n",
    "    print(f\"  üü¢ A FAVOR (+): {', '.join(top_positivos)}\")\n",
    "    print(f\"  üî¥ EN CONTRA (-): {', '.join(top_negativos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0defd-5882-4077-b11c-e56cd1045539",
   "metadata": {},
   "source": [
    "## 4.3 Qualitative Failure Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8219f-32dc-45ae-be4a-c95a9a9c9145",
   "metadata": {},
   "source": [
    "Qualitative Failure Analysis: Manual categorization of at least 5 specific misclassified examples (e.g., sarcasm, negation failure, ambiguity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954effc6-4f2a-4c69-9006-4bd11622bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Encontrar √≠ndices donde hay AL MENOS UN ERROR (Falso Positivo o Falso Negativo)\n",
    "# Comparamos fila por fila y_test contra y_pred\n",
    "errores_idx = np.where(np.any(y_test != y_pred, axis=1))[0]\n",
    "\n",
    "print(f\"Total de textos con al menos un error: {len(errores_idx)}\")\n",
    "print(\"Mostrando 5 ejemplos de fallos para tu Qualitative Analysis:\\n\")\n",
    "\n",
    "# 2. Escoger 5 √≠ndices al azar (usamos semilla para que siempre salgan los mismos)\n",
    "np.random.seed(42) \n",
    "ejemplos_mostrar = np.random.choice(errores_idx, 5, replace=False)\n",
    "\n",
    "# 3. Imprimir los textos originales y el fallo\n",
    "for i, idx in enumerate(ejemplos_mostrar):\n",
    "    # Cogemos el texto original (sin lematizar) para que sea legible\n",
    "    texto_original = X_test[idx]\n",
    "    \n",
    "    # Etiquetas que anotaron los humanos (Ground Truth)\n",
    "    real_labels = [class_names[j] for j, val in enumerate(y_test[idx]) if val == 1]\n",
    "    \n",
    "    # Etiquetas que predijo tu modelo\n",
    "    pred_labels = [class_names[j] for j, val in enumerate(y_pred[idx]) if val == 1]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"‚ùå EJEMPLO DE FALLO #{i+1}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"üìù TEXTO ORIGINAL:\\n{texto_original}\\n\")\n",
    "    \n",
    "    # Mostramos qu√© deb√≠a predecir vs qu√© predijo en realidad\n",
    "    print(f\"üß† VALORES REALES (Anotadores): {real_labels if real_labels else 'Ninguno'}\")\n",
    "    print(f\"ü§ñ VALORES PREDICHOS (Modelo):  {pred_labels if pred_labels else 'Ninguno'}\")\n",
    "    print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2dfbef-ae46-4dbe-890d-9d54bcbf259e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065deef-da30-4fbf-913b-be85ce08baa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (ML Master)",
   "language": "python",
   "name": "py313ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
