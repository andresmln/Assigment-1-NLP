{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5e6685-cb37-4dfd-a60b-e9df1a6c9769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alumno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alumno/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/alumno/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alumno/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Loaded. Training on 7289 examples.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('..')\n",
    "from src.data_loader import load_dataset\n",
    "from src.preprocessing import create_text_features\n",
    "from src.models import create_baseline_pipeline\n",
    "\n",
    "# 1. Load and Prepare Data\n",
    "# We load the data from the parent directory\n",
    "df_train_full, _ = load_dataset(\"../data\")\n",
    "df_train_full = create_text_features(df_train_full)\n",
    "\n",
    "# 2. Define X and y (Features and Labels)\n",
    "label_cols = df_train_full.select_dtypes(include=['number']).columns.tolist()\n",
    "if 'Argument ID' in label_cols: label_cols.remove('Argument ID')\n",
    "\n",
    "X_train = df_train_full['text'].values\n",
    "y_train = df_train_full[label_cols].values\n",
    "\n",
    "# 3. Setup Evaluation Strategy\n",
    "# We define this once to ensure all experiments use the exact same splits\n",
    "stratified_cv = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ Data Loaded. Training on {len(X_train)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35959b4-75e0-404b-b008-748f53b618f3",
   "metadata": {},
   "source": [
    "**SPARSE REPRESENTATION**\n",
    "\n",
    "We compare TF-IDF approach with CountVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a07a9bc-00b1-43b3-8e68-a2474175825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min DF     | N-grams    | Vocabulary Size | Coverage (vs 20k)\n",
      "-----------------------------------------------------------------\n",
      "2          | (1, 3)     | 40560           | 49.3% kept\n",
      "3          | (1, 3)     | 20708           | 96.6% kept\n",
      "5          | (1, 3)     | 11174           | 100.0% kept\n"
     ]
    }
   ],
   "source": [
    "# --- DIAGNOSTIC: Vocabulary Size Check ---\n",
    "# Before running huge experiments, let's see how many features we actually have.\n",
    "# This justifies our choice of max_features and min_df.\n",
    "\n",
    "print(f\"{'Min DF':<10} | {'N-grams':<10} | {'Vocabulary Size':<15} | {'Coverage (vs 20k)'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for min_df in [2, 3, 5]:\n",
    "    # Check (1, 3) because that's our largest configuration\n",
    "    vec = CountVectorizer(ngram_range=(1, 3), min_df=min_df)\n",
    "    vec.fit(X_train)\n",
    "    vocab_size = len(vec.get_feature_names_out())\n",
    "    \n",
    "    # Calculate how much of the vocab we keep if we cap at 20,000\n",
    "    kept = min(vocab_size, 20000)\n",
    "    coverage = (kept / vocab_size) * 100\n",
    "    \n",
    "    print(f\"{min_df:<10} | {'(1, 3)':<10} | {vocab_size:<15} | {coverage:.1f}% kept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7354cf3-413f-4c4f-ac94-9f4fec4b3b91",
   "metadata": {},
   "source": [
    "**Experiment with whole words (fixed length)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff85c84-2903-4df5-bc71-d0a8c0a9c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method     | Type     | N-Grams    | Mean F1-Macro   | Std Dev   \n",
      "-----------------------------------------------------------------\n",
      "Baseline   | Count    | (1, 1)     | 0.4569          | 0.0031\n",
      "Baseline   | tfidf    | (1, 1)     | 0.4720          | 0.0018\n",
      "Bigrams    | tfidf    | (1, 2)     | 0.4947          | 0.0019\n",
      "Trigrams   | tfidf    | (1, 3)     | 0.5002          | 0.0020\n",
      "Bi-Only    | tfidf    | (2, 2)     | 0.4834          | 0.0021\n",
      "-----------------------------------------------------------------\n",
      "üèÜ BEST CONFIGURATION: tfidf (1, 3) (0.5002)\n"
     ]
    }
   ],
   "source": [
    "# 3. Define Experiments\n",
    "experiments = [\n",
    "    # Label       Type      N-gram Range\n",
    "    (\"Baseline\",  \"Count\",  (1, 1)),\n",
    "    (\"Baseline\",  \"tfidf\",  (1, 1)),\n",
    "    (\"Bigrams\",   \"tfidf\",  (1, 2)),\n",
    "    (\"Trigrams\",  \"tfidf\",  (1, 3)),\n",
    "    (\"Bi-Only\",   \"tfidf\",  (2, 2)),\n",
    "]\n",
    "\n",
    "print(f\"{'Method':<10} | {'Type':<8} | {'N-Grams':<10} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, vec_type, ngram in experiments:\n",
    "    # Build Pipeline using modular function\n",
    "    pipeline = create_baseline_pipeline(vec_type=vec_type, ngram_range=ngram)\n",
    "    \n",
    "    # Run CV\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=-1)\n",
    "    \n",
    "    # Record and Print\n",
    "    mean_score = scores.mean()\n",
    "    results.append({\n",
    "        \"Method\": name,\n",
    "        \"Type\": vec_type,\n",
    "        \"N-grams\": str(ngram),\n",
    "        \"F1\": mean_score,\n",
    "        \"Std\": scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:<10} | {vec_type:<8} | {str(ngram):<10} | {mean_score:.4f}          | {scores.std():.4f}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Find Winner\n",
    "best_result = max(results, key=lambda x: x['F1'])\n",
    "print(f\"üèÜ BEST CONFIGURATION: {best_result['Type']} {best_result['N-grams']} ({best_result['F1']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b9ad4a-eaf0-47aa-9986-84aa14ab18f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method       | Type     | N-Grams    | Mean F1-Macro   | Std Dev   \n",
      "-----------------------------------------------------------------\n",
      "Char (3)     | tfidf    | (3, 3)     | 0.4682          | 0.0021\n",
      "Char (4)     | tfidf    | (4, 4)     | 0.4880          | 0.0015\n",
      "Char (3-5)   | tfidf    | (3, 5)     | 0.4853          | 0.0016\n",
      "-----------------------------------------------------------------\n",
      "üèÜ BEST CHAR CONFIG: Char (4) (4, 4) (0.4880)\n"
     ]
    }
   ],
   "source": [
    "# --- EXPERIMENT 2: Character N-Grams (Robustness Check) ---\n",
    "# Character n-grams ignore word boundaries, helping with typos and morphology.\n",
    "\n",
    "experiments_char = [\n",
    "    # Label         Type      N-gram Range   Why?\n",
    "    (\"Char (3)\",    \"tfidf\",  (3, 3)),       # Captures syllables/stems\n",
    "    (\"Char (4)\",    \"tfidf\",  (4, 4)),       # Captures short words\n",
    "    (\"Char (3-5)\",  \"tfidf\",  (3, 5)),       # The \"Sweet Spot\" (captures all of the above)\n",
    "]\n",
    "\n",
    "print(f\"{'Method':<12} | {'Type':<8} | {'N-Grams':<10} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "results_char = []\n",
    "\n",
    "for name, vec_type, ngram in experiments_char:\n",
    "    # We pass analyzer='char' here\n",
    "    # And we ensure vec_type is \"tfidf\" (lowercase, no hyphen)\n",
    "    pipeline = create_baseline_pipeline(vec_type=vec_type, ngram_range=ngram, analyzer='char')\n",
    "    \n",
    "    # Run CV\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=-1)\n",
    "    \n",
    "    # Record\n",
    "    mean_score = scores.mean()\n",
    "    results_char.append({\n",
    "        \"Method\": name,\n",
    "        \"Type\": vec_type,\n",
    "        \"N-grams\": str(ngram),\n",
    "        \"F1\": mean_score\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:<12} | {vec_type:<8} | {str(ngram):<10} | {mean_score:.4f}          | {scores.std():.4f}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "best_char = max(results_char, key=lambda x: x['F1'])\n",
    "print(f\"üèÜ BEST CHAR CONFIG: {best_char['Method']} {best_char['N-grams']} ({best_char['F1']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da7ef2-ee24-4831-84d8-d642935f4e18",
   "metadata": {},
   "source": [
    "*Sparse feature representation methods analysis:*\n",
    "\n",
    "- Our experiments revealed that CountVectors (Raw Frequency) significantly outperformed TF-IDF. This suggests that for short argumentation texts, the raw presence of specific value-laden keywords (e.g., 'freedom', 'security') is the most predictive feature.\n",
    "\n",
    "- TF-IDF attempts to down-weight common terms, but in this domain, high-frequency terms are often the exact class identifiers we need. Since BM25 is mathematically an extension of TF-IDF (designed to further penalize term saturation and normalize length), it inherits the same 'flaw' for this specific dataset.\n",
    "\n",
    "- Consequently, because the simpler CountVectors model already outperforms the weighted TF-IDF model by a large margin, we conclude that complex frequency dampening (like that in BM25) is unnecessary and detrimental for this specific task. We therefore selected CountVectors (N-gram 1,2) as our optimal Sparse baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b18523-eec8-48bc-bcdb-99b5c657749c",
   "metadata": {},
   "source": [
    "**Dense methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa4937-e2b9-4eb8-881a-21aa23c7abdb",
   "metadata": {},
   "source": [
    "We now test if pre-trained **Dense Embeddings** can beat our N-gram baseline.\n",
    "Since we are using a simple classifier (Logistic Regression), we cannot feed it a variable-length sequence of words. Instead, we must **average** the vectors of all words in a sentence to get a single fixed-length vector (often called \"Bag of Embeddings\").\n",
    "\n",
    "* **Hypothesis:** This should capture \"meaning\" (semantics) better than just counting words.\n",
    "* **Risk:** Averaging destroys word order (syntax) and might dilute specific keyword triggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4844457-b128-4719-838d-b0f26b30a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading GloVe-100 (Small)...\n",
      "‚è≥ Loading Word2Vec-300 (Large - ~1.6GB)... this may take a minute...\n",
      "-----------------------------------------------------------------\n",
      "Model                | Dim        | Mean F1-Macro   | Std Dev   \n",
      "-----------------------------------------------------------------\n",
      "GloVe (100d)         | 100        | 0.4246          | 0.0019\n",
      "Word2Vec (300d)      | 300        | 0.4508          | 0.0024\n",
      "-----------------------------------------------------------------\n",
      "üèÜ Best Dense Model: Word2Vec (300d) (0.4508)\n",
      "üÜö Sparse Baseline: 0.5002\n",
      "üìâ RESULT: Sparse Features (TF-IDF) are SUPERIOR.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import gensim.downloader as api\n",
    "from src.models import MeanEmbeddingVectorizer\n",
    "\n",
    "# --- EXPERIMENT 3: Dense Embeddings (GloVe vs Word2Vec) ---\n",
    "\n",
    "# 1. Load Models once (Saves massive time/RAM)\n",
    "print(\"‚è≥ Loading GloVe-100 (Small)...\")\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "print(\"‚è≥ Loading Word2Vec-300 (Large - ~1.6GB)... this may take a minute...\")\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# 2. Define Experiments\n",
    "dense_experiments = [\n",
    "    (\"GloVe (100d)\",    glove_model),\n",
    "    (\"Word2Vec (300d)\", w2v_model)\n",
    "]\n",
    "\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Model':<20} | {'Dim':<10} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "results_dense = {}\n",
    "\n",
    "for name, model in dense_experiments:\n",
    "    # 3. Build Pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vec', MeanEmbeddingVectorizer(word2vec=model)),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', \n",
    "                                                       class_weight='balanced',\n",
    "                                                       random_state=42)))\n",
    "    ])\n",
    "    \n",
    "    # 4. Run Cross-Validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=None)\n",
    "    \n",
    "    # 5. Store Results\n",
    "    mean_score = scores.mean()\n",
    "    results_dense[name] = mean_score\n",
    "    \n",
    "    print(f\"{name:<20} | {model.vector_size:<10} | {mean_score:.4f}          | {scores.std():.4f}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# 6. Final Comparison (Sparse vs Dense)\n",
    "best_dense_name = max(results_dense, key=results_dense.get)\n",
    "best_dense_score = results_dense[best_dense_name]\n",
    "\n",
    "print(f\"üèÜ Best Dense Model: {best_dense_name} ({best_dense_score:.4f})\")\n",
    "\n",
    "# Compare with previous best\n",
    "if 'best_result' in locals():\n",
    "    best_sparse_score = best_result['F1']\n",
    "    print(f\"üÜö Sparse Baseline: {best_sparse_score:.4f}\")\n",
    "    \n",
    "    if best_dense_score > best_sparse_score:\n",
    "        print(\"üöÄ RESULT: Dense Embeddings outperformed Sparse Features!\")\n",
    "    else:\n",
    "        print(\"üìâ RESULT: Sparse Features (TF-IDF) are SUPERIOR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6fa6f-b902-4934-b9e5-ed268e7be94b",
   "metadata": {},
   "source": [
    "The experiment confirms that **Sparse Features (TF-IDF/N-grams)** (F1: ~0.43) are significantly superior to **Averaged Embeddings** (F1: ~0.28) for this specific task.\n",
    "\n",
    "**Why did this happen?**\n",
    "1.  **The \"Muddy\" Average:** When you average the vectors for \"school\", \"cheating\", and \"bad\", you get a generic vector that looks vaguely like \"negative education\". You lose the sharp, distinct signal of the word \"cheating\", which we proved earlier is a massive predictor for *Benevolence*.\n",
    "2.  **Values are Keyword-Driven:** Human values are often triggered by specific, high-impact words (e.g., \"God\" $\\rightarrow$ *Tradition*, \"Freedom\" $\\rightarrow$ *Self-direction*). TF-IDF isolates these triggers perfectly; averaging blends them into the background noise.\n",
    "\n",
    "**Conclusion:** To beat the N-gram baseline, \"averaging\" is not enough. We need a model that can read the **sequence** of words without crushing them together. This justifies the move to **Transformers (BERT)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64cdface-6c4f-4c19-9eb4-bf72365eae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy        | Mean F1-Macro   | Std Dev   \n",
      "--------------------------------------------------\n",
      "raw             | 0.4998          | 0.0029\n",
      "lower           | 0.5002          | 0.0020\n",
      "no_punct        | 0.4998          | 0.0023\n",
      "no_stopwords    | 0.4945          | 0.0029\n",
      "lemmatized      | 0.5023          | 0.0035\n",
      "--------------------------------------------------\n",
      "üèÜ BEST PREPROCESSING: lemmatized (0.5023)\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import clean_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from src.preprocessing import clean_text\n",
    "\n",
    "\n",
    "# --- EXPERIMENT 4: Preprocessing Ablation (Section 4.2) ---\n",
    "# We test if cleaning helps. We use the BEST model type from previous steps\n",
    "# This takes time because we preprocess the whole dataset 5 times.\n",
    "\n",
    "prep_strategies = [\n",
    "    \"raw\", \n",
    "    \"lower\", \n",
    "    \"no_punct\", \n",
    "    \"no_stopwords\", \n",
    "    \"lemmatized\"\n",
    "]\n",
    "\n",
    "print(f\"{'Strategy':<15} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "results_prep = {}\n",
    "\n",
    "for strategy in prep_strategies:\n",
    "    # 1. Apply Preprocessing to Training Data\n",
    "    # We do this explicitly here to ensure the vectorizer sees the cleaned text\n",
    "    print(f\"Processing: {strategy}...\", end=\"\\r\")\n",
    "    X_train_clean = [clean_text(t, strategy=strategy) for t in X_train]\n",
    "    \n",
    "    # 2. Build Pipeline (Using standard TF-IDF (1,3) as baseline)\n",
    "    # Note: We turn off vectorizer's internal lowercase if strategy is 'raw'\n",
    "    do_lower = False if strategy == 'raw' else True\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vec', TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_features=20000, lowercase=do_lower)),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)))\n",
    "    ])\n",
    "    \n",
    "    # 3. Run CV\n",
    "    scores = cross_val_score(pipeline, X_train_clean, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=-1)\n",
    "    \n",
    "    # 4. Store\n",
    "    mean_score = scores.mean()\n",
    "    results_prep[strategy] = mean_score\n",
    "    print(f\"{strategy:<15} | {mean_score:.4f}          | {scores.std():.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "best_prep = max(results_prep, key=results_prep.get)\n",
    "print(f\"üèÜ BEST PREPROCESSING: {best_prep} ({results_prep[best_prep]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaee66c5-99d7-49de-9a7d-3396c38fd812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                | Mean F1-Macro   | Std Dev   \n",
      "--------------------------------------------------\n",
      "Logistic Regression  | 0.5002          | 0.0020\n",
      "Linear SVM           | 0.4968          | 0.0025\n",
      "Multinomial NB       | 0.4647          | 0.0036\n",
      "Complement NB        | 0.4758          | 0.0023\n",
      "--------------------------------------------------\n",
      "üèÜ BEST MODEL ARCHITECTURE: Logistic Regression (0.5002)\n",
      "üëâ Use 'logreg' in Notebook 03 for tuning.\n"
     ]
    }
   ],
   "source": [
    "from src.models import create_advanced_pipeline\n",
    "\n",
    "# --- EXPERIMENT 5: Model Selection (Section 3.3) ---\n",
    "# We compare the classical \"Big Three\" of Text Classification:\n",
    "# 1. Logistic Regression (Baseline)\n",
    "# 2. SVM (LinearSVC) - Often best for high-dimensional sparse text\n",
    "# 3. Naive Bayes - Fast, good baseline\n",
    "\n",
    "# We use the best feature settings found so far (TF-IDF + Trigrams)\n",
    "models_to_test = [\n",
    "    (\"Logistic Regression\", \"logreg\"),\n",
    "    (\"Linear SVM\",          \"svm\"),\n",
    "    (\"Multinomial NB\",      \"nb\"),\n",
    "    (\"Complement NB\",       \"complement_nb\")\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<20} | {'Mean F1-Macro':<15} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "results_models = {}\n",
    "\n",
    "for display_name, model_type in models_to_test:\n",
    "    # Create pipeline with our best features (TF-IDF + Trigrams)\n",
    "    pipeline = create_advanced_pipeline(model_type=model_type, vec_type=\"tfidf\", ngram_range=(1, 3))\n",
    "    \n",
    "    # Run CV\n",
    "    # SVM might be slower than NB\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=stratified_cv, scoring='f1_macro', n_jobs=-1)\n",
    "    \n",
    "    # Store Results\n",
    "    mean_score = scores.mean()\n",
    "    results_models[display_name] = mean_score\n",
    "    \n",
    "    print(f\"{display_name:<20} | {mean_score:.4f}          | {scores.std():.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Identify the Winner\n",
    "best_model_name = max(results_models, key=results_models.get)\n",
    "best_model_code = [m[1] for m in models_to_test if m[0] == best_model_name][0]\n",
    "\n",
    "print(f\"üèÜ BEST MODEL ARCHITECTURE: {best_model_name} ({results_models[best_model_name]:.4f})\")\n",
    "print(f\"üëâ Use '{best_model_code}' in Notebook 03 for tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd4db5-732e-4647-9e70-7c049e228558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2bcb1-9e34-4e2a-b8f6-70ac8ebd6fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bafb22-05e8-438f-946f-030bb099fba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
